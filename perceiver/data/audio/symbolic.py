import functools
import os
import random
from pathlib import Path
from typing import Dict, List, Optional

import numpy as np
import pytorch_lightning as pl
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader

from perceiver.data.audio.midi_processor import encode_midi_files


class SymbolicAudioDataModule(pl.LightningDataModule):
    _EXAMPLE_SEPARATOR_INPUT_ID = -1
    _PAD_INPUT_ID = 388
    _VOCAB_SIZE = 389

    def __init__(
        self,
        dataset_dir: str,
        max_seq_len: int,
        min_seq_len: Optional[int] = None,
        padding_side: str = "left",
        batch_size: int = 16,
        num_workers: int = 1,
        preproc_workers: Optional[int] = None,
        pin_memory: bool = True,
    ):
        """Base class for data preprocessing and loading across different audio data sources using MIDI as the
        source data format.

        :param dataset_dir: Directory for storing the preprocessed dataset.
        :param max_seq_len: Maximum sequence length generated by this data module.
        :param min_seq_len: Minimum sequence length generated by this data module. If set the length of each sequence
                            will be randomly chosen from the interval [min_seq_len, max_seq_len].
        :param padding_side: Padding side for sequences that are shorter than the configured max_seq_len. Can be set to
                             "left" or "right".
        :param batch_size: Batch size of loaded training data.
        :param num_workers: Number of data loading processes.
        :param preproc_workers: Number of preprocessing processes. If not defined, defaults to `num_workers`.
        """
        super().__init__()
        if min_seq_len is not None and not (0 < min_seq_len < max_seq_len):
            raise ValueError(
                "Invalid data configuration supplied. "
                "Parameter 'min_seq_len' must adhere to 0 < min_seq_len < max_seq_len."
            )

        self.save_hyperparameters()

        self._collator = SymbolicAudioCollator(
            max_seq_len=self.hparams.max_seq_len + 1,
            pad_token=self._PAD_INPUT_ID,
            padding_side=self.hparams.padding_side,
        )
        self._ds_train = None
        self._ds_valid = None

    @property
    def vocab_size(self):
        return self._VOCAB_SIZE

    @property
    def max_seq_len(self):
        return self.hparams.max_seq_len

    @property
    def preproc_workers(self):
        if self.hparams.preproc_workers is not None:
            return self.hparams.preproc_workers
        else:
            return max(1, self.hparams.num_workers)

    @property
    @functools.lru_cache(maxsize=1)
    def preproc_dir(self) -> Path:
        return Path(self.hparams.dataset_dir) / "preproc"

    @property
    def train_data_file(self) -> Path:
        return self.preproc_dir / "train.bin"

    @property
    def valid_data_file(self) -> Path:
        return self.preproc_dir / "valid.bin"

    def prepare_data(self) -> None:
        if not os.path.exists(self.preproc_dir):
            dataset = self.load_source_dataset()

            encoded_train = self._encode_midi_files(dataset["train"])
            encoded_valid = self._encode_midi_files(dataset["valid"])

            random.shuffle(encoded_train)
            data_train = self._to_flattened_numpy_array(encoded_train)
            data_valid = self._to_flattened_numpy_array(encoded_valid)

            self.preproc_dir.mkdir(parents=True)
            self._save_memory_map(data=data_train, target_file=self.train_data_file)
            self._save_memory_map(data=data_valid, target_file=self.valid_data_file)

    def load_source_dataset(self) -> Dict[str, Path]:
        """Must return a dictionary with keys 'train' and 'valid' referring to directories containing the training
        and validation data."""
        raise NotImplementedError("`load_source_dataset` must return a dictionary with keys 'train' and 'valid'.")

    def _encode_midi_files(self, dataset_dir: Path) -> List[np.ndarray]:
        if not dataset_dir.exists():
            raise ValueError(f"Invalid directory supplied. Directory '{dataset_dir}' does not exist.")

        midi_files = list(dataset_dir.rglob("**/*.mid")) + list(dataset_dir.rglob("**/*.midi"))
        return encode_midi_files(midi_files, num_workers=self.preproc_workers)

    def _to_flattened_numpy_array(self, input_ids_list: List[np.ndarray]) -> np.ndarray:
        input_ids = [np.append(input_ids, [self._EXAMPLE_SEPARATOR_INPUT_ID]) for input_ids in input_ids_list]
        return np.concatenate(input_ids)

    @staticmethod
    def _save_memory_map(data: np.ndarray, target_file: Path) -> None:
        fp = np.memmap(str(target_file.absolute()), dtype=np.int16, mode="w+", shape=data.shape)
        fp[:] = data[:]
        fp.flush()

    def setup(self, stage: str = None) -> None:
        self._ds_train = SymbolicAudioNumpyDataset(
            data_file=str(self.train_data_file),
            max_seq_len=self.hparams.max_seq_len + 1,
            separator_input_id=self._EXAMPLE_SEPARATOR_INPUT_ID,
            min_seq_len=self.hparams.min_seq_len + 1 if self.hparams.min_seq_len is not None else None,
        )
        self._ds_valid = SymbolicAudioNumpyDataset(
            data_file=str(self.valid_data_file),
            max_seq_len=self.hparams.max_seq_len + 1,
            separator_input_id=self._EXAMPLE_SEPARATOR_INPUT_ID,
        )

    def train_dataloader(self):
        return DataLoader(
            self._ds_train,
            shuffle=False,
            collate_fn=self._collator,
            batch_size=self.hparams.batch_size,
            num_workers=self.hparams.num_workers,
            pin_memory=self.hparams.pin_memory,
        )

    def val_dataloader(self):
        return DataLoader(
            self._ds_valid,
            shuffle=False,
            collate_fn=self._collator,
            batch_size=self.hparams.batch_size,
            num_workers=self.hparams.num_workers,
            pin_memory=self.hparams.pin_memory,
        )


class SymbolicAudioNumpyDataset(torch.utils.data.Dataset):
    def __init__(self, data_file: str, max_seq_len: int, separator_input_id: int, min_seq_len: Optional[int] = None):
        self._data = np.memmap(data_file, dtype=np.int16, mode="r")
        self._max_seq_len = max_seq_len
        self._separator_input_id = separator_input_id
        self._min_seq_len = min_seq_len
        self._length = self._data.shape[0] // self._max_seq_len

    def __getitem__(self, index):
        start_pos = torch.randint(self._data.shape[0] - self._max_seq_len, (1,)).item()
        sample = torch.tensor((self._data[start_pos : start_pos + self._max_seq_len]).astype(np.int64))

        if self._separator_input_id not in sample:
            example = sample
        else:
            examples = list(
                torch.tensor_split(sample, torch.where(sample == self._separator_input_id)[0])
            )  # split at example separators
            examples = sorted(examples, key=lambda x: len(x), reverse=True)  # order by longest sample

            example = examples[0]
            example = example[example != self._separator_input_id]

        if self._min_seq_len is not None and self._min_seq_len < len(example):
            chunk_length = torch.randint(self._min_seq_len, self._max_seq_len, (1,)).item()
            example = example[:chunk_length]

        return {"input_ids": example}

    def __len__(self):
        return self._length


class SymbolicAudioCollator:
    def __init__(self, max_seq_len: int, pad_token: int, padding_side: str):
        self._max_seq_len = max_seq_len
        self._pad_token = pad_token
        self._padding_side = padding_side

    def __call__(self, input_batch):
        input_ids = []
        pad_masks = []

        for example in input_batch:
            inp, mask = self._pad(example["input_ids"])
            input_ids.append(inp)
            pad_masks.append(mask)

        batch = torch.stack(input_ids, dim=0)
        return (
            batch[..., 1:],
            batch[..., :-1],
            torch.stack(pad_masks, dim=0)[..., :-1].type(torch.bool),
        )

    def _pad(self, x):
        if len(x) == self._max_seq_len:
            return x, torch.zeros(x.shape)

        pad_size = self._max_seq_len - len(x)

        if self._padding_side == "left":
            pad = (pad_size, 0)
        elif self._padding_side == "right":
            pad = (0, pad_size)
        else:
            raise ValueError(f"Invalid padding side '{self._padding_side}'")

        padded = F.pad(x, pad, "constant", self._pad_token)
        pad_mask = torch.zeros(padded.shape)
        pad_mask = pad_mask.masked_fill(padded == self._pad_token, 1)
        return padded, pad_mask
